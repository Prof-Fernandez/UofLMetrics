---
title: "Linear Regression Review"
subtitle: "MSBA Data Analytics III"
author: "Jose M. Fernandez"
institute: "University of Louisville"
date: "2020-7-4 (updated: `r Sys.Date()`)"
output:
  xaringan::moon_reader:
    css: [default, uol, uol-fonts, "extra.css"]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---
## Simple Linear Regression: Introduction

- We are interested in the causal relationship between two random variables \(X\) and \(Y\)
- We assume that \(X\) (the regressor or the independent variable) causes changes in \(Y\) (the dependent variable)
- We assume that the relationship between them is linear
- We are interested in estimating the slope of this linear relationship

---
## Example: The Effect of Class Size on Student Performance (1)

- Suppose the superintendent of an elementary school district wants to know the effect of class size on student performance
- She decides that test scores are a good measure of performance

```{r echo=FALSE, warning=FALSE, message=FALSE}
library(knitr)
include_graphics("https://thecadreupei.com/wp-content/uploads/2019/11/exam-696x462.jpg")

```


---
## The Effect of Class Size on Student Performance (2)

What she wants to know is what is the value of $$\beta_{ClassSize} = \frac{\text{change in }TestScore}{\text{change in }ClassSize} = \frac{\Delta TestScore}{\Delta ClassSize}$$
If we knew $\beta_{ClassSize}$ we could answer the question of how student test scores would change in response to a specific change in the class sizes 
$$\Delta TestScore = \beta_{ClassSize} \times \Delta ClassSize$$
If $\beta_{ClassSize} = -0.6$ then a reduction in class size by two students would yield a change of test scores of $(-0.6)\times(-2) = 1.2$.

---
## Linear Equation of Relationship (1)

$\beta_{ClassSize}$ would be the slope of the straight line describing the linear relationship between $TestScore$ and $ClassSize$

$$TestScore = \beta_0 + \beta_{ClassSize} \times ClassSize$$

If we knew the parameters $\beta_0$ and $\beta_{ClassSize}$, not only would we be able to predict the change in student performance, we would be able to predict the average test score for any class size
---
## Linear Equation of Relationship (2)

We can't predict exact test scores since there are many other factors that influence them that are not included

- teacher quality
- better textbooks 
- different student populations 
- etc.

Our linear equation in this case is

$$TestScore = \underbrace{\beta_0 + \beta_{ClassSize} \times ClassSize}_{\text{Average }TestScore} + \text{ other factors}$$
---
## General Form

More generally, if we have $n$ observations for $X_i$ and $Y_i$ pairs 

$$Y_i = \beta_0 + \beta_1 X_i + u_i$$


- $\beta_0$ (intercept) and $\beta_1$ (slope) are the model parameters 
- $\beta_0 + \beta_1 X_i$ is the population regression line (function)
- $u_i$ is the error term. 
- It contains all the other factors beyond $X$ that influence $Y$. We refer to this relationship as: $Y_i$ regressed on $X_i$
- In our case, $Y_i$ is the average test score and $X_i$ is the average class size, for district $i$.
---
## Regression Model Plot
```{r  out.width = "100%",fig.align = "center", echo=FALSE}
library(knitr)
include_graphics("ch4pic1.png") 
```
---
## Parameter Estimation

Our model

$$Y_i = \beta_0 + \beta_1 X_i + u_i$$

We don't know the parameters $\beta_0$ and $\beta_1$

But we can use the data we have available to infer the value of these parameters

- Point estimation
- Confidence interval estimation
- Hypothesis testing
---
## Class Size Data

To estimate the model parameters of the class size/student performance model we have data from 420 California school districts in 1999

![](ch4pic2.png)
---
## Correlation and Scatterplot (1)

- The sample correlation is found to be -0.23, indicating a weak negative relationship. 
- However, we need a better measure of causality 
- We want to be able to draw a straight line through these dots characterizing the linear regression line, and from that we get the slope.
---
## Correlation and Scatterplot (2)

```{r out.width="50%",fig.align='center', warning=FALSE,message=FALSE}
suppressPackageStartupMessages(library(AER,quietly=TRUE,warn.conflicts=FALSE))
library(ggplot2,quietly=TRUE,warn.conflicts=FALSE)
data("CASchools")
CASchools$str=CASchools$students/CASchools$teachers
CASchools$testscr=(CASchools$read+CASchools$math)/2
qplot(x=str, y=testscr, data=CASchools, geom="point",
      xlab="Student/Teacher Ratio", ylab="Test Scores")
```
---
## Correlation and Scatterplot (3)
```{r echo=FALSE, out.width="100%"}
include_graphics("linear-regression.png")
```

---
## Estimators

You are already familiar with one estimator for a series, __the mean__.

Suppose you have n observations of $y_i$. 
$$y_1,y_2,y_3,...,y_n$$
If I have you guess the next value what would it be? 

Let's call this guess $\hat{y}$

The difference between $y_i - \hat{y}=error= u_i$

An estimator minimizes the size of the error.
---
### Estimator (2)

One way to capture the size of the error is to use the variance of the error. 

$$\frac{1}{n}\sum_i^n (u_i-\bar{u})^2 = \frac{1}{n}\sum_i^n u_i^2=\frac{1}{n}\sum_i^n (y_i-\hat{y})^2$$
Let's assume that the mean of the errors is zero. How can we pick $\hat{y}$ to make this sum as small as possible (minimization problem)?
---
### Estimator (3)
We can solve for the minimum using calculus.

$$\frac{d}{d\hat{y}}\frac{1}{n}\sum_i^n (y_i-\hat{y})^2=\frac{-2}{n}\sum_i^n (y_i-\hat{y})=0 \\ \frac{1}{n}\sum_i^n (y_i-\hat{y})=0 \\ \frac{1}{n}\sum_i^n y_i-\hat{y}=\bar{y}-\hat{y}=0 \\ \bar{y} = \hat{y}$$ 
For regression, we are essentially doing the same thing.
---
## The Ordinary Least Squares Estimator (1)

- The _ordinary least squares (OLS)_ estimator selects estimates for the model parameters that minimize the distance between the sample regression line (function) and the observed data.
- Recall that we use $\bar{Y}$ as an estimator of $E[Y]$ since it minimizes $\sum_i (Y_i - m)^2$
- With OLS we are interested in minimizing 
$$\min_{b_0,b_1} \sum_i [Y_i - (b_0 + b_1 X_i)]^2$$ 
We want to find $b_0$ and $b_1$ such that the mistakes between the observed $Y_i$ and the predicted value $b_0 + b_1 X_i$ are minimized.

__See the notes for the derivation__
---
## The Ordinary Least Squares Estimator (2)

The OLS estimator of $\beta_1$ is 
$$\hat{\beta}_1 = \frac{\sum_i (X_i - \bar{X})(Y_i - \bar{Y})}{\sum_i (X_i - \bar{X})^2} = \frac{s_{XY}}{s_X^2}$$

The OLS estimator of $\beta_0$ is 
$$\hat{\beta}_0 = \bar{Y} - \hat{\beta}_1 \bar{X}$$

The predicted value of $Y_i$ is 
$$\hat{Y}_i = \hat{\beta}_0 + \hat{\beta}_1 X_i$$

The error in predicting $Y_i$ is called the residual $$\hat{u}_i = Y_i - \hat{Y}_i$$
---
## OLS Regression of Test Scores on Student-Teacher Ratio (1)

Using data from the 420 school districts an OLS regression is run to estimate the relationship between test score and teacher-student ratio (STR).

$$\widehat{TestScore} = 698.9 - 2.28 \times STR$$

where $\widehat{TestScore}$ is the predicted value. (This is referred to as test scores regressed on STR)
---
## OLS Regression of Test Scores on Student-Teacher Ratio (2)
```{r out.width="50%",fig.align='center'}
qplot(x=str, y=testscr, data=CASchools, geom="point", xlab="Student/Teacher
      Ratio", ylab="Test Scores") + geom_abline(intercept=698.9, slope=-2.28,
                                                color='blue')
```
---
## Goodness of Fit

Now that we've run an OLS regression we want to know

- How much does the regressor account for variation in the dependent variable?
- Are the observations tightly clustered around the regression line?

We have two useful measures

- The regression $R^2$
- The standard error of the regression $(SER)$
---
## The R Squared (1)

  __The $R^2$ is the fraction of the sample variance of $Y_i$ (dependent variable) explained by $X_i$ (regressor)__

- From the definition of the regression predicted value $\hat{Y}_i$ we can write $$Y_i = \hat{Y}_i + \hat{u}_i$$ and $R^2$ is the ratio of the sample variance of $\hat{Y}_i$ and the sample variance of $Y_i$
- $R^2$ ranges from 0 to 1. $R^2 = 0$ indicates that $X_i$ has no explanatory power at all, while $R^2 = 1$ indicates that it explains $Y_i$ fully.
---
## The R Squared (2)

Let us define the total sum of squares $(TSS)$, the explained sum of squares $(ESS)$, and the sum of squared residuals $(SSR)$

$$\begin{align*} Y_i &= \hat{Y}_i + \hat{u}_i \\ Y_i - \bar{Y} &= \hat{Y}_i - \bar{Y} + \hat{u}_i \\ (Y_i - \bar{Y})^2 &= (\hat{Y}_i - \bar{Y} + \hat{u}_i)^2 \\ (Y_i - \bar{Y})^2 &= (\hat{Y}_i - \bar{Y})^2 + (\hat{u}_i)^2 + 2(\hat{Y}_i - \bar{Y})\hat{u}_i \\ \underbrace{\sum_i(Y_i - \bar{Y})^2}_{TSS} &= \underbrace{\sum_i(\hat{Y}_i - \bar{Y})^2}_{ESS} + \underbrace{\sum_i(\hat{u}_i)^2}_{SSR} + \underbrace{2\sum_i(\hat{Y}_i - \bar{Y})\hat{u}_i}_{=0} \\ TSS &= ESS + SSR \end{align*}$$
---
## The R Squared (3)

$$TSS = ESS + SSR$$

$R^2$ can be defined as $$R^2 = \frac{ESS}{TSS} = 1 - \frac{SSR}{TSS}$$
---
## The Standard Error of the Regression

The standard error of the regression $(SER)$ is an estimator of the standard deviation of the population regression error $u_i$.

Since we don't observe $u_1,\dots, u_n$ we need to estimate this standard deviation
We use $\hat{u}_1,\dots, \hat{u}_n$ to calculate our estimate

$$SER = s_{\hat{u}}$$ where $$s_{\hat{u}}^2 = \frac{1}{n-2}\sum_i \hat{u}_i^2 = \frac{SSR}{n-2}$$
---
## Measure of Fit of Test Score on STR Regression

- The $R^2$ is calculated to be 0.051. This means that the $STR$ explains 5.1% of the variance in $TestScore$.
- The $SER$ is calculated to be 18.6. This is an estimate of the standard deviation of $u_i$ which shows a large spread.
- Low $R^2$ (or low $SER$) does not mean that the regression is bad: it means that there are other factors that have a strong influence on the dependent variable that have not been included in the regression.
---
## Regression in R

**Load the data**

If you haven't done so already, then let's load the data into R.
We want the CA Schools data from the AER library.

Enter the following code into R

- library(AER)
- data(CASchools)

Let's Run a Regression
```{r message=FALSE, warning=FALSE, results='hide'}
library(knitr)
library(stargazer)
regress.results=lm(formula = testscr ~ str, data=CASchools)
```
---
## Regression in R 
```{r message=FALSE, warning=FALSE,results='asis'}
stargazer(regress.results, type = "html", font.size="tiny", no.space =TRUE, star.char = c("c","b","a"), notes = c("c","b","a"), dep.var.labels.include = FALSE, column.labels = "Test Score", covariate.labels = "Student-Teacher Ratio", style = "apsr", omit.table.layout = "n", single.row = TRUE)
```
---