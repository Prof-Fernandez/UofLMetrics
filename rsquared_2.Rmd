---
title: "Causal Inference"
subtitle: "Simple Linear Regression"
author: "Jose Fernandez"
institute: "University of Louisville"
date: "2020/6/29 (updated: `r Sys.Date()`)"
output:
  xaringan::moon_reader:
    css: [default, uol, uol-fonts, "extra.css"]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---
class: center, inverse, middle
# Simple Linear Regression

---
## Goodness of Fit

Now that we've run an OLS regression we want to know

- How much does the regressor account for variation in the dependent variable?
- Are the observations tightly clustered around the regression line?

We have two useful measures

- The regression $R^2$
- The standard error of the regression ($SER$)

---
## The R Squared (1)

  __The $R^2$ is the fraction of the sample variance of $Y_i$ (dependent variable) explained by $X_i$ (regressor)__

- From the definition of the regression predicted value $\hat{Y}_i$ we can write $$Y_i = \hat{Y}_i + \hat{u}_i$$ and $R^2$ is the ratio of the sample variance of $\hat{Y}_i$ and the sample variance of $Y_i$
- $R^2$ ranges from 0 to 1. $R^2 = 0$ indicates that $X_i$ has no explanatory power at all, while $R^2 = 1$ indicates that it explains $Y_i$ fully.

---
## The R Squared (2)

Let us define the total sum of squares ($TSS$), the sum of squares Regression ($SSR$), and the sum of squared Error ($SSE$)

$$\begin{align*} Y_i &= \hat{Y}_i + \hat{u}_i \\ Y_i - \bar{Y} &= \hat{Y}_i - \bar{Y} + \hat{u}_i \\ (Y_i - \bar{Y})^2 &= (\hat{Y}_i - \bar{Y} + \hat{u}_i)^2 \\ (Y_i - \bar{Y})^2 &= (\hat{Y}_i - \bar{Y})^2 + (\hat{u}_i)^2 + 2(\hat{Y}_i - \bar{Y})\hat{u}_i \\ \underbrace{\sum_i(Y_i - \bar{Y})^2}_{TSS} &= \underbrace{\sum_i(\hat{Y}_i - \bar{Y})^2}_{SSR} + \underbrace{\sum_i(\hat{u}_i)^2}_{SSE} + \underbrace{2\sum_i(\hat{Y}_i - \bar{Y})\hat{u}_i}_{=0} \\ TSS &= SSR + SSE \end{align*}$$

---
## The R Squared (3)

$$TSS = SSR + SSE$$

$R^2$ can be defined as $$R^2 = \frac{SSR}{TSS} = 1 - \frac{SSE}{TSS}$$

---
## The Standard Error of the Regression

The standard error of the regression $(SER)$ is an estimator of the standard deviation of the population regression error $u_i$.

Since we don't observe $u_1,\dots, u_n$ we need to estimate this standard deviation
We use $\hat{u}_1,\dots, \hat{u}_n$ to calculate our estimate

$$SER = s_{\hat{u}}$$ where $$s_{\hat{u}}^2 = \frac{1}{n-k-1}\sum_i \hat{u}_i^2 = \frac{SSE}{n-k-2}=MSE$$
Mean Squared Error (MSE) is the estimated variance of the error term. Noticed that the denominator is $n-k-1$ where $k$ is the number of regressors. This is just like the sample variance of x, except $k=0$ in that case. In the case of simple linear regression $k=1$.

---
## Mean Squared Regression and the F-test

The mean squared regression tells us the average of the variation explained by the regression slopes.

If we want to know whether or not the regression line is better than just using the mean. We need to conduct an F-test. The F-test assumes that all of the regression slopes are equal to zero.

$$F = \frac{MSR}{MSE}$$
Remember the F statistic is the ration of two variances. SSE is a variance and so is SSR. The degrees of freedom in the numerator are $k$ and the degrees of freedom in the denominator are $n-k-1$.
---
## Measure of Fit of Test Score on STR Regression

- The $R^2$ is calculated to be 0.051. This means that the $STR$ explains 5.1\% of the variance in $TestScore$.
- The $SER$ is calculated to be 18.6. This is an estimate of the standard deviation of $u_i$ which shows a large spread.
- Low $R^2$ (or low $SER$) does not mean that the regression is bad: it means that there are other factors that have a strong influence on the dependent variable that have not been included in the regression.

---
class: center, inverse, middle
# Regression in R

---
## Load the data
If you haven't done so already, then let's load the data into R.
We want the CA Schools data from the AER library.

Enter the following code into R

- library(AER)
- data(CASchools)

---
## Let's run a regression
```{r warning=FALSE, message=FALSE}
library(AER)
data(CASchools)
CASchools$str=CASchools$students/CASchools$teachers
CASchools$testscr=(CASchools$read+CASchools$math)/2
regress.results=lm(formula = testscr ~ str, data=CASchools)
summary(regress.results)
```

---
## Here is the Annova Table
```{r}
anova(regress.results)
```

---
## Let's make it table look nice
```{r warning=FALSE, message=FALSE, results='asis'}
# My favorite package is stargazzer. Be sure to include the option results = 'asis'
library(stargazer)
stargazer(regress.results, type="html", single.row=TRUE, covariate.labels = c("Student-Teacher Ratio"))
```

---
## Let's make it look nice
```{r warning=FALSE, message=FALSE, results='asis'}
# Another option is sjplot. Be sure to include the option results = 'asis'
library(sjPlot)
tab_model(regress.results, show.se=TRUE, pred.labels =c( "Constant","Student-Teacher Ratio"), dv.labels = "test scores")
```

---
## Let's make it look nice
```{r warning=FALSE, message=FALSE}
library(broom)
library(knitr)
kable(tidy(regress.results))
```

---
## What do I need to know for the Quiz

Your first quiz is multiple choice and leans heavily on the book. If you have not read the book chapters, then I implore on you to do so.

Topics to be covered
- Experiments
    - Why do we use experiments?
      - They give us unbiased estimates controlling for both observed and unobserved factors.
    - Why can't we use experiments on everything?
      - It is sometimes unethical or impossible to run certain experiments.
      - The experiment may be too expensive to run (Basic Income)
      - We do not randomly assign pregnant mothers to smoke so we can study the effects of smoking on birth outcomes.
    - When can experiments produce bias results?
      - Hawthorne effect
      - Attrition Bias
      - Sample Selection bias

  
---
## What do I need to know for the Quiz


- Simple Linear Regression
  - Know the assumptions
  - Know how to interpret the Betas
  - Know Cov(X,Y)/Var(X) gives you the slope
  - know $\bar{Y}-\beta_1 \bar{X}$ is the regression constant.
  - What does it mean the $\beta$`s are unbiased?
  - Know what a confidence interval is.
  - Know the difference between endogenous, exogenous, and correlation

---
## OLS Assumptions

1. The parameters ($\beta$`s) are linear in the equation.
2. Y and X are chosen at random from a population
3. $E[e|X]=0$, this assumption holds two important features.
  - The mean of the error is zero.
  - The error term is not correlated with X (i.e. independent/exogenous)
4. $Var[e|x]=\sigma^2$, the variance of the error term is constant for all x values.
