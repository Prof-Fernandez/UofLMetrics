---
title: "Limited Dependent Variables"
subtitle: "Censored and Truncated Data"
author: "Jose M. Fernandez"
institute: "University of Louisville"
date: "2020-7-4 (updated: `r Sys.Date()`)"
output:
  xaringan::moon_reader:
    css: [default, uol, uol-fonts, "extra.css"]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---
## Censored Data

In the world of data science sometimes data do not appear the way we would like.

One such case is when data are either _top coded_ or _bottom coded_.

For example, you may be interested in household income. You run to the American Community Survey only to find the following.

* Household earning less than $10,000 are given a lumped into one income category (i.e. _bottom coded_).
* Household earning more than $1,000,000 are given a lumped into the top income cateogry (i.e. _top coded_).

These variables are missing due to exogenous reasons.
---
![](https://www.researchgate.net/profile/Cheryl-Wachenheim/publication/23514232/figure/tbl1/AS:667777467686916@1536221963529/2-Income-levels-of-survey-participants.png)
We can use the midpoint for the in between values, but not the top or bottom categories.
---
## Censored Data

A second type of censored data is when the data are missing for endogenous reasons.

For example, a person's wage information is missing because they are not in the labor force. However, this does not mean that if we could magically put them in the labor force that their wage would be zero.

Simply, the person's __reservation wage__ is higher than the wage offers they are receiving.
---
![](https://libertystreeteconomics.newyorkfed.org/wp-content/uploads/sites/2/2022/12/LSE_2022_SCE-labor_kosar_ch1.png)
---
## Censored Data

A different example, consider you want to know the effect of High School GPA on College GPA. You will only observe a college GPA for people who get into college. Likewise, if you sample college students, then you will observe college GPA, but you will only observe the high school GPA of individuals who entered college.

AVG HS GPA given you went to college is 2.71. 

AVG HS GPA given you did not go to college is 2.04.
---
## Truncated Data

A third problem that can exist is that not only can you not observe the dependent variable, but you also cannot observed the explanatory variables.

In this case, if both the dependent and independent variables are censored due to a cutoff value, then we say the data are truncated.
---
## Censored Data

Therefore, we need three different types of models to deal with _exogenous_ missing data and _endogenous_ missing data.

Tobit - __exogenously__ censored dependent variable.

Truncated Regression - __exogenous__ censoring of both dependent and independent variables.

Sample Selection Model (Heckman Model) - __endogenously__ censored dependent variable
---
## Tobit

Suppose Y (wages) are subject to "top coding" (as is often observed in government data):
$$Y=\left\{\begin{matrix}
y^* & y^*<c \\ 
c & y^*>c
\end{matrix}\right.$$

We are interested in estimating the equation $$Y=X\beta+u$$

If we are willing to assume $u \rightarrow N(0,\sigma^2)$, then we can use the Tobit Model
---
## Tobit Density and Likelihood Function

The probability density of Y will resemble that of a normal distribution.
$$f(y)=\left\{\begin{matrix}
\frac{1}{\sigma}\phi(\frac{y_i-X_i\beta}{\sigma}) & y^*<c \\ 
1-\Phi(\frac{c-X_i\beta}{\sigma}) & y^*>c
\end{matrix}\right.$$

Log Likelihood Function $$LL(\beta,\sigma)=\sum_{c>y} \frac{1}{\sigma}\phi(\frac{y_i-X_i\beta}{\sigma})+\sum_{c<y} 1-\Phi(\frac{c-X_i\beta}{\sigma})$$
---
## Bottom Coding

The equation is very similar for the bottom coding situation.
__Bottom coding__ means we are censored from below.
$$Y=\left\{\begin{matrix}
y^* & y^*>c \\ 
c & y^*<c
\end{matrix}\right.$$
The likelihood function changes to capture this difference in bottom coding
Log Likelihood Function $$LL(\beta,\sigma)=\sum_{c<y} \frac{1}{\sigma}\phi(\frac{y_i-X_i'beta}{\sigma})+\sum_{c>y} \Phi(\frac{c-X_i'beta}{\sigma})$$
---
![](https://live.staticflickr.com/6227/6289592741_d2bd3ca766_z.jpg)
---
## The Expected Value of the Tobit model
Let's assume your y variable is bottom coded at zero. This means we only observe Y if Y>0.

The unconditional expected value is of Y is
$$E[Y_i]=\Phi(\frac{x_i \beta}{\sigma})(x_i \beta+\sigma *\lambda) \\ \lambda = \frac{\phi(x_i \beta/\sigma)}{\Phi(x_i \beta/\sigma)}$$
If we condition on Y>0, then the expected value simplifies to 

$$E[Y_i|Y_i>0,x]=x_i \beta+\sigma *\lambda \\ \lambda = \frac{\phi(x_i \beta/\sigma)}{\Phi(x_i \beta/\sigma)}$$
The $\lambda$ term is the Inverse Mills Ratio.

---
## Marginal Effects of Tobit Model

We can calculate the probability of Y>0
$$Pr(Y>0)=\Phi(x_i \beta/\sigma) \\ \frac{d Pr(Y>0)}{dx} = \frac{\beta}{\sigma}*\phi(x_i \beta/\sigma)$$
Similarly, we can calculate the marginal effect of x on Y.
$$\frac{d E[Y]}{dx} = \beta*\Phi(x_i \beta/\sigma)$$

---
## Tobit Model in R
```{r eval=FALSE, message=FALSE, warning=FALSE}
library(AER)
library(stargazer)
data("Affairs")
fm.ols <- lm(affairs ~ age + yearsmarried + religiousness + occupation
             + rating,  data = Affairs)
fm.tobit <- tobit(affairs ~ age + yearsmarried + religiousness + occupation 
                  + rating,  data = Affairs)
fm.tobit2 <- tobit(affairs ~ age + yearsmarried + religiousness + occupation 
                   + rating,  right = 4, data = Affairs)
stargazer(fm.ols,fm.tobit,fm.tobit2,type="html")
```
---
## Tobit Model in R 
```{r echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
library(knitr)
library(AER)
library(stargazer)
library(tidyverse)
data("Affairs")
fm.ols <- lm(affairs ~ age + yearsmarried + religiousness + occupation + rating,  data = Affairs)
fm.tobit <- tobit(affairs ~ age + yearsmarried + religiousness + occupation + rating,  data = Affairs)
fm.tobit2 <- tobit(affairs ~ age + yearsmarried + religiousness + occupation + rating,  right = 4, data = Affairs)
stargazer(fm.ols,fm.tobit,fm.tobit2,type="html",omit.table.layout = "sn", model.numbers = FALSE, font.size = "small", single.row = TRUE, star.char = c("c","b","a"), style = "qje")
```
---
## Marginal effects with Tobit
Let's use the `censReg` package to estimate the tobit model and get marginal effects.
```{r results='asis', warning=FALSE, message=FALSE}
library(censReg)
estResult <- censReg( affairs ~ age + yearsmarried + religiousness + occupation + rating, data = Affairs )
margEff(estResult) %>% kable()
```

---
class: center, inverse, middle
# Truncated Regression
---
## Truncated Regression

Truncated data differ from censored data in that we only observe y __and__ x if y is above (below) a certain cutoff. In censored data, we always observed x.

Example: Hausman and Wise's analysis of the New Jersey negative income tax experiment (Truncated Regression)

Goal: Estimate earnings function for low income individuals

Truncation: Individuals with earnings greater than 1.5 poverty level were excluded from the sample.

Two types of inference:

1. Inference about entire population in presence of truncation
2. Inference about sub-population observed
---
## Truncated Regression Model
$$y_i=x_i'\beta+e_i, i=1, ..., n$$
where the error term $e_i \rightarrow N(0,\sigma^2)$

Truncation from below: observe $y_i$ and $x_i$ if $y_i>c$

Truncation from above: observe $y_i$ and $x_i$ if $y_i<c$
---
## Truncated Normal Distribution
Let X be a normally distributed random variable with mean $\mu$ and variance $\sigma^2$. Also, let there be a cutoff constant $c$.

Then we can write the probability density function of X to be 
$$\begin{align*}f(X|X>c) &= \frac{f(x)}{Pr(X>c)}=\frac{f(x)}{1-F(c)} \\ \\&= \frac{\frac{1}{\sigma}\phi\frac{x-\mu}{\sigma}}{1-\Phi(\frac{c-\mu}{\sigma})}\end{align*}$$
---
## Truncated Normal Distribution
The mean and variance are given by 
$$\begin{align*}E[X|X>c] &= \mu+\sigma \lambda(\frac{c-\mu}{\sigma}) \\ var(X|X>c) &= \sigma^2(1-\delta(\frac{c-\mu}{\sigma})) \end{align*}$$

where $\lambda(z)=\frac{\phi(z)}{1-\Phi(z)}$ = Inverse Mills Ratio 

and $\delta(z)=\lambda(z)(\lambda(z)-z),\,0<\delta(z)<1$


---
## Truncated Normal Distribution

Consequences of not accounting for the truncation is that you will overstate the mean and understate the variance. __WHY?__

We are throwing out lower values pushing the mean up.

We are throwing out values from the extreme part of the distribution, which means we have less variation in the remaining data.

---
## Truncated Normal Distribution

![](truncnorm.png)

---
## Estimate of Truncated Normal
Simply using Ordinary Least Squares will result in a bias.

Instead, we use maximum likelihood. We can correct for the missing data by dividing the probability density function by the Pr(y>c).

$$f(y|y>c) =\frac{\frac{1}{\sigma}\phi\frac{y-x'\beta}{\sigma}}{1-\Phi(\frac{c-x'\beta}{\sigma})}$$
Notice that $\int_c^\infty f(y|y>c) dy=1$

Likelihood function $$L(\beta,\sigma^2|x,y)=\prod_i^n \frac{\frac{1}{\sigma}\phi\frac{y-x'\beta}{\sigma}}{1-\Phi(\frac{c-x'\beta}{\sigma})}$$
---
## Truncated Regression in R
```{r}
########################
## Artificial example ##
########################
## simulate a data.frame
set.seed(1071)
n <- 10000
sigma <- 4
alpha <- 2
beta <- 1
x <- rnorm(n, mean = 0, sd = 2)
eps <- rnorm(n, sd = sigma)
y <- alpha + beta * x + eps
d <- data.frame(y = y, x = x)
```
---
## Truncated Regression in R
```{r message=FALSE, warning=FALSE}
library(truncreg) # Truncated Regression package
## truncated response
d$yt <- ifelse(d$y > 1, d$y, NA)
## compare estimates for full/truncated/censored/threshold response
fm_full <- lm(y ~ x, data = d)
fm_ols <- lm(yt ~ x, data = d)
fm_trunc <- truncreg(yt ~ x, data = d, point = 1, direction = "left")
```
---
## OLS vs Truncreg
```{r echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
stargazer::stargazer(fm_full,fm_ols,fm_ols,type="html",dep.var.labels = c("Full Data","Truncated"),column.labels = c("OLS","OLS","Trunc. Reg."),coef=list(NULL,NULL,fm_trunc$coefficients[1:2]),se = list(NULL,NULL,sqrt(diag(fm_trunc$vcov[1:2,1:2]))),omit.stat = c("rsq","adj.rsq","F","ser"), model.numbers = FALSE, font.size = "small", single.row = TRUE, star.char = c("c","b","a"), omit.table.layout = "sn")
```
---
## Truncated Regression: Histogram
```{r message=FALSE, warning=FALSE, eval=FALSE}
group1<-rep("Normal",length(d$y))
group2<-rep("Truncated",length(d$yt))
data_1<-data.frame(c(d$y,d$yt),c(group1,group2))
names(data_1)<-c("Y","groups")
library(ggplot2)
ggplot(data_1, aes(x=Y, fill=groups)) +
     geom_density(alpha=.3)
```
---
## Truncated Regression: Histogram
```{r message=FALSE, warning=FALSE, echo=FALSE}
group1<-rep("Normal",length(d$y))
group2<-rep("Truncated",length(d$yt))
data_1<-data.frame(c(d$y,d$yt),c(group1,group2))
names(data_1)<-c("Y","groups")
library(ggplot2)
ggplot(data_1, aes(x=Y, fill=groups)) +
     geom_density(alpha=.3)
```
---
class: center, middle, inverse
# Endogenous Censoring
---
## Types of Sampling Bias
- Non-response Bias
- Survivorship Bias
- Attrition Bias
- Self-Selection Bias
- Undercoverage Bias
- Exclusion Bias
- Convenience Sampling
- Cherry picking

---
## Non-response Bias
![](https://images.prismic.io/sketchplanations/f2fdb7cb-f126-4897-ad78-4fd11c743172_SP+723+-+Sampling+bias.png?auto=format&ixlib=react-9.0.3&h=1887.6833845104059&w=1600&q=35&dpr=3)

---
## Survivorship Bias
![](https://images.prismic.io/sketchplanations/ce000ac3-b8f1-4612-8c86-28f5d65442f7_SP+674+-+Survivorship+bias.png?auto=format&ixlib=react-9.0.3&h=1887.557603686636&w=1600&q=35&dpr=3)

---
## Attrition Bias
![](https://images.prismic.io/sketchplanations/c24b9624-d973-4bf4-9c29-b248005ab56e_78697608699.jpg?auto=format&ixlib=react-9.0.3&h=1888.2591093117408&w=1600&q=35&dpr=3)

---
## Heckman Model

The Heckman Model can be thought of as a two part model (but it can be done in one step)

Step 1. You estimate a probit/logit model that determines if the dependent variable is censored.

Step 2. Construct the Inverse Mills Ratio (corrects for sample selection)

Step 3. Run the regression including the Inverse Mills Ratio as an explanatory variable. 
---
## Heckman Model: Math

Consider the model
$$\begin{align*}Y &= X\beta +\sigma u \\
d &=1(z'\gamma+v>0) \end{align*}$$

where z contains an instrumental variable not contained in X. The error terms $v$ and $u$ are assumed to be jointly distributed standard normal. $$\binom{u}{v}\to N\left ( 0,\begin{bmatrix} 1 & \rho \\ \rho & 1 \end{bmatrix} \right )$$
---
## Heckman Model: Math

We want to estimate the 

$$\begin{align*}E[Y|X,d=1] &= X\beta+\sigma E[u|d=1] \\
&= X\beta+\sigma E[u|z'\gamma+v>0]  \\
&= X\beta+\sigma \rho \lambda(z'\gamma) \end{align*}$$

where $\lambda(z\gamma)$ is called the _Inverse Mills Ratio_ $$\lambda(z\gamma)=\frac{\phi(z'\gamma)}{\Phi(z'\gamma)}$$
Notice, this is the ratio between the _normal pdf_ and the _normal cdf_ of our probit coefficients with the person's X's. 
---
## Heckman Two Step

First, we estimate the decision equation to get our estimates of $\gamma$.

We use those estimates to construct $\lambda(z'\widehat{\gamma})$.

Then we estimate the level equation as $$Y=X\beta+\alpha\lambda(z'\widehat{\gamma})+u$$

If $\alpha=0$, then there is no sample selection problem.
---
## OLS versus Heckman

Now is a good time to think about how Heckman corrects OLS.

Let's restate the omitted variable bias (OVB) table, but using the correction factor as the OV. If x affects both the participation equation and the level equation, then we will have some sort of OVB.

Signs      | $Corr(x,\text{Mills Ratio})>0$ | $Corr(x,\text{Mills Ratio})<0$
---------  | ------------------- | ------------------
$\alpha>0$| Positive Bias       | Negative Bias
$\alpha<0$| Negative Bias       | Positive Bias
---
## Simulation Example
```{r message=FALSE, warning=FALSE, echo=TRUE, eval=FALSE}
library(tidyverse)
library(sampleSelection)
set.seed(123456)
N = 10000
educ = sample(1:16, N, replace = TRUE)
age  = sample(18:64, N, replace = TRUE)
covmat = matrix(c(.46^2*4, .5*.46*2, .5*.46*2, 1), ncol = 2)
errors = mvtnorm::rmvnorm(N, sigma = covmat)
z = rnorm(N)
e = errors[, 1]
v = errors[, 2]
wearnl = 4.49 + .08 * educ + .12 * age + e
d_star = -1.5 + 0.15 * educ + 0.01 * age + 0.15 * z + v

observed_index  = d_star > 0
d = data.frame(wearnl, educ, age, z, observed_index)
# lm based on full data
lm_all = lm(wearnl ~ educ + age, data=d)

# lm based on observed data
lm_obs = lm(wearnl ~ educ + age, data=d[observed_index,])
# Sample Selection model
selection_2step = selection(observed_index ~ educ + age + z, wearnl ~ educ + age, method = '2step')
stargazer::stargazer(lm_all,lm_obs,selection_2step, type = "html", font.size = "tiny", single.row = TRUE, star.char = c("c","b","a"), style = "qje", model.numbers = FALSE, omit.stat = c("rsq","f","adj.rsq","ser"))
```
---
## Simulation Example
<small>
```{r message=FALSE, warning=FALSE, echo=FALSE, eval=TRUE, results='asis'}
library(tidyverse)
library(sampleSelection)
set.seed(123456)
N = 10000
educ = sample(1:16, N, replace = TRUE)
age  = sample(18:64, N, replace = TRUE)
covmat = matrix(c(.46^2*4, .5*.46*2, .5*.46*2, 1), ncol = 2)
errors = mvtnorm::rmvnorm(N, sigma = covmat)
z = rnorm(N)
e = errors[, 1]
v = errors[, 2]
wearnl = 4.49 + .08 * educ + .12 * age + e
d_star = -1.5 + 0.15 * educ + 0.01 * age + 0.15 * z + v

observed_index  = d_star > 0
d = data.frame(wearnl, educ, age, z, observed_index)
# lm based on full data
lm_all = lm(wearnl ~ educ + age, data=d)

# lm based on observed data
lm_obs = lm(wearnl ~ educ + age, data=d[observed_index,])
# Sample Selection model
selection_2step = selection(observed_index ~ educ + age + z, wearnl ~ educ + age, method = '2step')
stargazer::stargazer(lm_all,lm_obs,selection_2step, type = "html", font.size = "tiny", single.row = TRUE, star.char = c("c","b","a"), style = "qje", model.numbers = FALSE, omit.stat = c("rsq","f","adj.rsq","ser"))
```
</small>
---
## Heckman Model in R
```{r eval=FALSE, message=FALSE, warning=FALSE}
library(sampleSelection)
data( Mroz87 )
Mroz87$kids  <- ( Mroz87$kids5 + Mroz87$kids618 > 0 )
# Two-step estimation
# heckit(first stage equation, second stage equation, data, method c("2step","ml"))
heck.m1 <- heckit( lfp ~ age + I( age^2 ) + faminc + kids + educ,
   wage ~ exper + I( exper^2 ) + educ + city, Mroz87, method = "2step" ) 
# MLE Estimator
heck.m2 <- heckit( lfp ~ age + I( age^2 ) + faminc + kids + educ,
   wage ~ exper + I( exper^2 ) + educ + city, Mroz87, method = "ml" ) 
# OLS Estimator
ols.m1 <- lm(wage ~ exper + I( exper^2 ) + educ + city, Mroz87)
stargazer::stargazer(ols.m1,heck.m1,heck.m2,type="html", model.numbers = FALSE, font.size = "small", single.row = TRUE, star.char = c("c","b","a"), omit.table.layout = "sn")
```

---
<small>
```{r echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
library(sampleSelection)
data( Mroz87 )
Mroz87$wage[Mroz87$wage==0]<-NA
Mroz87$kids  <- ( Mroz87$kids5 + Mroz87$kids618 > 0 )
# Two-step estimation
heck.m1 <- heckit( lfp ~ age + I( age^2 ) + faminc + kids + educ,
   wage ~ exper + I( exper^2 ) + educ + city, Mroz87, method = "2step" ) 
# MLE Estimator
heck.m2 <- heckit( lfp ~ age + I( age^2 ) + faminc + kids + educ,
   wage ~ exper + I( exper^2 ) + educ + city, Mroz87, method = "ml" ) 
# OLS Estimator
ols.m1 <- lm(wage ~ exper + I( exper^2 ) + educ + city, Mroz87)
stargazer::stargazer(ols.m1,heck.m1,heck.m2,type="html", omit.stat = c("rsq","f","adj.rsq","ser"), model.numbers = FALSE, font.size = "small", single.row = TRUE, star.char = c("c","b","a"), style = "qje")
```
</small>
---
### The two step method 
We can do the two step method by brute force as well.

```{r}
first.stage<-glm(lfp ~ age + I( age^2 ) 
                 + faminc + kids + educ , data=Mroz87, 
                 family=binomial(link = "probit"))
Mroz87$z1 <- predict.glm(first.stage)
Mroz87$millsratio <- dnorm(Mroz87$z1)/(pnorm(Mroz87$z1))
second.stage <- lm(wage ~ exper + I( exper^2 ) 
                   + educ + city+millsratio, Mroz87)
```
---
<small>
```{r echo=FALSE, results='asis'}
stargazer(second.stage,heck.m1,type="html", single.row = TRUE)
```
</small>
---
## Summary
```{r fig.align='center',echo=FALSE,out.width="100%"}
knitr::include_graphics("summarypic.jpg")
```
---
## Summary
```{r fig.align='center',echo=FALSE,out.width="100%"}
knitr::include_graphics("censored_pic.gif")
```
